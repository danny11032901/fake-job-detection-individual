

import nltk
from nltk.tokenize import word_tokenize

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Sample text
text = "The students are studying NLP in the lab."

# Tokenize
tokens = word_tokenize(text)

# Apply POS tagging
pos_tags = nltk.pos_tag(tokens)
print("POS Tags:", pos_tags)


# Additional tokenization examples
text1 = "Data Science is fun!"
text2 = "Hey my name is Danish"
text3 = "How is everyone doing"

tokens1 = word_tokenize(text1)
tokens2 = word_tokenize(text2)
tokens3 = word_tokenize(text3)

print("Tokens 1:", tokens1)
print("Tokens 2:", tokens2)
print("Tokens 3:", tokens3)
